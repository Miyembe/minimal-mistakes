---
title: "[Paper] Addressing Function Approximation Error in Actor-Critic Methods"
excerpt: "Twin Delayed Deep Deterministic Policy Gradient (TD3), an improved version of DDPG solving overestimation error."

categories:
  - paperoni
tags:
  - DRL

---
## Overview
This paper proposes a new version of DDPG algorithm, which is a successful actor-critic based algorithm widely used in continuous control. The origin DDPG algorithm exhibits stability issues which is caused by overstimation of value function approximator. The proposed method effectively dealt with the overstimation issue and added additional modification (delayed policy update, regularization, smoothing update). The reported result is amazingly good. The best part of this paper is that the author has provided its code and results really clearly in a GitHub Repository!
### Links
Paper: <https://arxiv.org/pdf/1802.09477.pdf>
Code: <https://github.com/sfujim/TD3> 
## Overestimation Bias
The main contribution of TD3 is that this method is that this method effectively reduced the overestimation bias. The summary of how this paper approached to this problem is summarised below:
* In Q-learning, Q value is updated by taking the current reward and the discounted maximum value of Q value in the next state.
* When the noise is injected during update, Q value with the addition of noise will be used for update, inducing overestimation in Q value. 
* This accumulates over training period, so trained approximated Q-value does not accurate to represent the true Q value. 
* This naturally leads to the inaccurate policy update. 

## Clipped Double Q-Learning for Actor-Critic
As the solution to relieve this overestimation bias, the author used a technique used in the Double Q-learning, which used two approximated Q value for better approximation. 

By selecting the minimum estimate from two Q value, the target is updated with the smaller value, which prevents overestimation. It can induce underestimation bias; however, it is much better as the underestimation bias is not propagated. It used a tiny technique to relieve computation burden compared the original Double Q-learning. Check this out in the paper. 

## Addressing Variance
In addition to the overestimation bias, this paper tried to another problem that harms training policy, high variance. 

By nature, TD-learning (Q learning) causes accumulating error. As it uses bootstrapping, estimating next Q-value, it causes TD-error. When calculating the expectied discounted sum of future rewards, the accumulated TD-error is injected in the estimated value. 

To solve this issue, target networks and delayed policy updates are used. Target networks are well-known to stabilitse training Q value, proposed by the famous DQN paper! This technique is used to reduce the accumulating TD error. 

This paper brought up another problem. In Actor-Critic setting, the policy updates is terrible with a high variance value estimate. This makes the divergence in critic updates, making vicious cycle of learning - failing training! 

The paper relieved this problem by slowing down updating policy - ** Reducing Divergence **.

## Target Policy Smoothing Regularization
As DPG has deterministic policy, overfitting problem can be caused as it is sucsceptible to inaccuracies induced by function approximation error, increasing the variance of the target. 

The author solved this problem with target policy smoothing. By injecting clipped value of the noise in the action, it smoothed the value estimating by bootstrapping off of similar state-action value estimates. The author stated that this style of update can additionally lead to improvement in stochastic domains with failure cases.  
